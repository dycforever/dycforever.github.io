<!DOCTYPE html>

<head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="base_format.css">
</head>

data: 20140823

<body>
    一个socket对应内核中的一个struct socket，它有一个类型为struct proto_ops的成员ops，表示从socket函数到传输层对应函数的映射，对于tcp就是inet_stream_ops()。
    
    一个inet_protosw对应一个传输层协议，一个协议族的所有inet_protosw都存在静态的inetsw_array[]中；
    而系统所有的inet_protocol都记录在inetsw[]中，每种type对应数组中的一项。
    ipv4的inetsw_array[]定义在net/ipv4/af_inet.c中，包含了SOCK_STREAM/SOCK_DGRAM/SOCK_RAW三种type的socket，ipv4的SOCK_STREAM也就是TCP协议。

    一般一个协议族里，每种类型的传输类型都对应一个确定的协议，但是将来也许有除了TCP以外的SOCK_STREAM类型协议，因此socket层的bind/listen等函数，在调用传输层SOCK_STREAM中对应实现时，也要通过sock-&gt;sk_prot转换一下，TCP的struct proto实例是tcp_prot。

    socket也是一种文件系统，通过cat /proc/filesystems可以看到sockfs，对应sock_fs_type，它的成员用于。
    由此可见，一个struct file_system_type对应一个文件系统类型，它有分配/释放超级块的函数，因为一种文件系统可以装在多个分区上。
    而每个超级块有struct super_operations，用来分配/释放inode的函数，socket的inode其实是一个socket_alloc，socket文件的操作集合是socket_file_ops。

    <h2>
        bind()
    </h2>
    socket函数都使用系统调用sys_socketcall()，以socket listen为例：
    sys_socketcall()    net/socket.c
        =&gt; sys_bind()      net/socket.c
            =&gt; socket-&gt;ops-&gt;bind()     net/socket.c
                =&gt; inet_bind()    net/ipv4/af_inet.c
                    =&gt; sk-&gt;sk_prot-&gt;get_port()   net/ipv4/inet_connection_sock.c
                        =&gt; tcp_v4_get_port() net/ipv4/tcp_ipv4.c
    tcp没有tcp_bind()，所以sk-&gt;sk_prot-&gt;bind()为空，所以具体操作都在inet_bind()中。
    tcp_v4_get_port()本质上就是找一个可用的端口（如果没有显式指定），然后往系统全局的tcp_hashinfo-&gt;bhash[]中添加一个inet_bind_bucket结构，表示此端口已被占用。
    同时也会调用inet_bind_hash()函数，将sock加入到inet_bind_bucket-&gt;owners链表中，并用sock-&gt;icsk_bind_hash指向inet_bind_bucket对象。(另一个会将sock加入inet_hash_bucket-&gt;owners的就成就是调用tcp_v4_connect())

    可以看到，bind()和listen()的过程中，都会调用tcp_v4_get_port()，这和下面设置inet_bind_bucket-&gt;fastreuse成员的逻辑有关：
    一个设置了reuse熟悉的port是可以被重用的（多次添加到tcp_hashinfo-&gt;bhash[]），但是必须大家都设置了reuse属性，且只能有一个是LISTEN。不过当reuse属性为2时是可以强行bind的！
    再看多个进程并发执行时怎么确定他们是否将bind的sock用于listen呢？将一个sock的sk_stat设置为TCP_LISTEN是发生在inet_csk_listen_start()中的，如果这个地方不调用tcp_v4_get_port()再次确认，就无法保证只有一个进程可以listen此port了。tb（也就是inet_bind_bucket对象）的并发安全就依靠inet_bind_bucket-&gt;lock这个自旋锁来保护。
    实验也验证了这个过程：
    多个进程是可以同时bind一个port的（调用bind()之前要先设置SO_REUSEADDR），但是同时listen就会报错。

    <h2>
        listen()
    </h2>
    sys_socketcall()    net/socket.c
        =&gt; sys_listen()      net/socket.c
            =&gt; socket-&gt;ops-&gt;listen()     net/socket.c
                =&gt; inet_listen()    net/ipv4/af_inet.c
                    =&gt; inet_csk_listen_start()    net/ipv4/inet_connection_sock.c
                        =&gt; reqsk_queue_alloc()   net/core/request_sock.c
                        =&gt; sk-&gt;sk_prot-&gt;get_port()   net/ipv4/inet_connection_sock.c
                            =&gt; tcp_v4_get_port() net/ipv4/tcp_ipv4.c
    inet_csk_listen_start()内会将sock的stat设置为TCP_LISTEN；一个监听socket对应一个inet_connection_sock，它有个成员icsq_accept_queue用来存储所有系统收到的建立连接请求。
    icsq_accept_queue中有成员resq_accept_head和rskq_accept_tail，表示已经完成三次握手，等待accept的连接；
	inet_listen()中会设置sk-&gt;sk_max_ack_backlog为backlog，表示建立连接队列的最大值。

    icsq_accept_queue还有个listen_sock类型的listen_opt成员，其中有一张syn_table hash表，用来存放所有接收到syn的半连接；
    listen_opt在reqsk_queue_alloc()中分配，syn_table的size是2^(backlog + 1)，同时把此hash表中可存放的半连接最大值lopt-&gt;max_qlen_log设置为backlog + 1；
    监听socket在接收到SYN后:
        1.用reqsk_queue_is_full()判断半连接队列(icsk_accept_queue)长度，里面也是用指数方法来判断是否超限。
        2.用sk_acceptq_is_full()判断已连接队列是否超过sk_max_ack_backlog

    <h2>
        accept()
    </h2>
    sys_socketcall()    net/socket.c
        =&gt; sys_accept()      net/socket.c
            =&gt; socket-&gt;ops-&gt;accept()     net/socket.c
                =&gt; inet_accept()    net/ipv4/af_inet.c
                    =&gt; sk-&gt;sk_prot-&gt;accept()   net/ipv4/af_inet.c
                        =&gt; inet_csk_accept() net/ipv4/inet_connection_sock.c

    <h2>
        setsockopt()
    </h2>
    以LINGER参数为例，grep内核源码可以发现，有两个带LINGER的enum，一个叫做SOCK_LINGER，另一个叫做TCP_LINGER2，从名字上可以明白，一个是socket层的参数，而另一个是tcp层的参数。应用在调用setsockopt()时通过level参数来表示设置哪一层的参数。
    socket层的SOCK_LINGER参数就是unp书中所描述的linger选项，设置它的调用栈如下：
        sys_setsockopt()  net/socket.c
            sock_setsockopt()  net/sock.c
        将时间存到sk-&gt;sk_lingertime中，然后设置sk-&gt;sk_flags中的SOCK_LINGER标志位。

    tcp层的TCP_LINGER2用来控制socket在FIN_WAIT2状态的停留时间，在tcp_keepalive_timer()中会用到，设置它的调用栈如下：
        sys_setsockopt()  net/socket.c
            sock-&gt;ops-&gt;setsockopt()
                sock_common_socksetopt()  net/ipv4/af_inet.c
                    sk-&gt;sk_prot-&gt;setsockopt()  
                        tcp_setsockopt()   net/ipv4/tcp_ipv4.c
                            tcp_do_setsockopt()   net/ipv4/tcp.c

    <h2>
        close()
    </h2>
    sys_close()    fs/open.c 
        filp_close()   fs/open.c
            fput()         fs/file_table.c
                filp-&gt;f_op-&gt;release()   fs/file_table.c
    
    sock_close()   net/socket.c
        sock_release()  net/socket.c
            sock-&gt;ops-&gt;release()  
                inet_release()    net/af_inet.c
                    sk-&gt;sk_prot-&gt;close()   
                        tcp_close()     net/tcp.c
    上文所述的linger参数会在inet_release()被读取，传递给tcp_close()函数，最终在sk_stream_wait_close()中起作用

struct inet_ehash_bucket {
	struct hlist_head chain;
	struct hlist_head twchain;
};
根据四元组找sock时一般会在chain和twchain里面都找一遍。

struct inet_bind_hashbucket {
	spinlock_t		lock;
	struct hlist_head	chain;
};

代码上面注释解释了什么时候，端口是可以共享的，并重点说明了设置sk-&gt;sk_reuse时的情况
struct inet_bind_bucket {
	unsigned short		port;
	signed short		fastreuse;
	struct hlist_node	node;
	struct hlist_head	owners;
};


</body>

inet_protosw: include/net/protocol.h
struct socket: include/linux/net.h
struct proto_ops: include/linux/net.h
struct file_system_type sock_fs_type: net/socket.c
struct socket_alloc: include/net/sock.h
struct file_operations socket_file_ops: net/socket.c


struct sock: include/net/sock.h
struct proto: include/net/sock.h
struct proto tcp_prot: net/ipv4/tcp_ipv4.c

现在还有两个问题：
reuse=2是什么情况
rwmem那3个参数到底是什么意思。
